{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: none\" align=\"center\">\n",
    "   <tr style=\"border: none\">\n",
    "      <th style=\"border: none\"><font face=\"verdana\" size=\"4\" color=\"black\"><b>  Demonstrate detection of adversarial samples using ART  </b></font></font></th>\n",
    "   </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate the detection of adversarial samples using ART. Our classifier will be a **ResNet** architecture for the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) image data set.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1.\t[Loading prereqs and data](#prereqs)\n",
    "2.  [Evaluating the classifier](#classifier)\n",
    "3.  [Training the detector](#train_detector)\n",
    "4.  [Evaluating the detector](#detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prereqs\"></a>\n",
    "## 1. Loading prereqs and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "from art import config\n",
    "from art.utils import load_dataset, get_file, preprocess\n",
    "from art.estimators.classification import KerasClassifier\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.defences.detector.evasion.subsetscanning import SubsetScanningDetector\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import six\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "NB_TRAIN = 100\n",
    "NB_TEST = 100\n",
    "\n",
    "__file__ = '/home/jinie/PycharmProjects/adversarial-robustness-toolbox/uilts/'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Load the CIFAR10 data set and class descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TYPE = Tuple[  # pylint: disable=C0103\n",
    "    Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray], float, float\n",
    "]\n",
    "\n",
    "def load_cifar10(\n",
    "    raw: bool = False,\n",
    ") -> DATASET_TYPE:\n",
    "    \"\"\"\n",
    "    Loads CIFAR10 dataset from config.CIFAR10_PATH or downloads it if necessary.\n",
    "\n",
    "    :param raw: `True` if no preprocessing should be applied to the data. Otherwise, data is normalized to 1.\n",
    "    :return: `(x_train, y_train), (x_test, y_test), min, max`\n",
    "    \"\"\"\n",
    "\n",
    "    def load_batch(fpath: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Utility function for loading CIFAR batches, as written in Keras.\n",
    "\n",
    "        :param fpath: Full path to the batch file.\n",
    "        :return: `(data, labels)`\n",
    "        \"\"\"\n",
    "        with open(fpath, \"rb\") as file_:\n",
    "            if sys.version_info < (3,):\n",
    "                content = six.moves.cPickle.load(file_)\n",
    "            else:\n",
    "                content = six.moves.cPickle.load(file_, encoding=\"bytes\")\n",
    "                content_decoded = {}\n",
    "                for key, value in content.items():\n",
    "                    content_decoded[key.decode(\"utf8\")] = value\n",
    "                content = content_decoded\n",
    "        data = content[\"data\"]\n",
    "        labels = content[\"labels\"]\n",
    "\n",
    "        data = data.reshape(data.shape[0], 3, 32, 32)\n",
    "        return data, labels\n",
    "\n",
    "    path = os.path.join('/home/jinie/Documents/cifar-10-batches-py/')\n",
    "    num_train_samples = 50000\n",
    "\n",
    "    x_train = np.zeros((num_train_samples, 3, 32, 32), dtype=np.uint8)\n",
    "    y_train = np.zeros((num_train_samples,), dtype=np.uint8)\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        fpath = os.path.join(path, \"data_batch_\" + str(i))\n",
    "        data, labels = load_batch(fpath)\n",
    "        x_train[(i - 1) * 10000 : i * 10000, :, :, :] = data\n",
    "        y_train[(i - 1) * 10000 : i * 10000] = labels\n",
    "\n",
    "    fpath = os.path.join(path, \"test_batch\")\n",
    "    x_test, y_test = load_batch(fpath)\n",
    "    y_train = np.reshape(y_train, (len(y_train), 1))\n",
    "    y_test = np.reshape(y_test, (len(y_test), 1))\n",
    "\n",
    "    # Set channels last\n",
    "    x_train = x_train.transpose((0, 2, 3, 1))\n",
    "    x_test = x_test.transpose((0, 2, 3, 1))\n",
    "\n",
    "    min_, max_ = 0.0, 255.0\n",
    "    if not raw:\n",
    "        min_, max_ = 0.0, 1.0\n",
    "        x_train, y_train = preprocess(x_train, y_train, clip_values=(0, 255))\n",
    "        x_test, y_test = preprocess(x_test, y_test, clip_values=(0, 255))\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test), min_, max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test), min_, max_ = load_cifar10()\n",
    "\n",
    "num_samples_train = 100\n",
    "num_samples_test = 100\n",
    "x_train = x_train[0:num_samples_train]\n",
    "y_train = y_train[0:num_samples_train]\n",
    "x_test = x_test[0:num_samples_test]\n",
    "y_test = y_test[0:num_samples_test]\n",
    "\n",
    "class_descr = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test), _, _ = load_dataset(\"mnist\")\n",
    "x_train, y_train = x_train[:NB_TRAIN], y_train[:NB_TRAIN]\n",
    "x_test, y_test = x_test[:NB_TEST], y_test[:NB_TEST]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classifier\"></a>\n",
    "## 2. Subset Scan Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _kr_weights_loader(dataset, weights_type, layer=\"DENSE\"):\n",
    "    import keras.backend as k\n",
    "\n",
    "    filename = str(weights_type) + \"_\" + str(layer) + \"_\" + str(dataset) + \".npy\"\n",
    "\n",
    "    def _kr_initializer(_, dtype=None):\n",
    "        weights = np.load(os.path.join(os.path.dirname(os.path.dirname(__file__)), \"utils/resources/models\", filename))\n",
    "        return k.variable(value=weights, dtype=dtype)\n",
    "\n",
    "    return _kr_initializer\n",
    "\n",
    "\n",
    "def get_image_classifier_kr(\n",
    "    loss_name=\"categorical_crossentropy\", loss_type=\"function_losses\", from_logits=False, load_init=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Standard Keras classifier for unit testing\n",
    "    The weights and biases are identical to the TensorFlow model in get_classifier_tf().\n",
    "    :param loss_name: The name of the loss function.\n",
    "    :type loss_name: `str`\n",
    "    :param loss_type: The type of loss function definitions: label (loss function defined by string of its name),\n",
    "                      function_losses (loss function imported from keras.losses), function_backend (loss function\n",
    "                      imported from keras.backend)\n",
    "    :type loss_type: `str`\n",
    "    :param from_logits: Flag if model should predict logits (True) or probabilities (False).\n",
    "    :type from_logits: `bool`\n",
    "    :param load_init: Load the initial weights if True.\n",
    "    :type load_init: `bool`\n",
    "    :return: KerasClassifier, tf.Session()\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "\n",
    "    tf_version = [int(v) for v in tf.__version__.split(\".\")]\n",
    "    if tf_version[0] == 2 and tf_version[1] >= 3:\n",
    "        is_tf23_keras24 = True\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        from tensorflow import keras\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "    else:\n",
    "        is_tf23_keras24 = False\n",
    "        import keras\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "    from art.estimators.classification.keras import KerasClassifier\n",
    "\n",
    "    # Create simple CNN\n",
    "    model = Sequential()\n",
    "\n",
    "    if load_init:\n",
    "        if is_tf23_keras24:\n",
    "            model.add(\n",
    "                Conv2D(\n",
    "                    1,\n",
    "                    kernel_size=(7, 7),\n",
    "                    activation=\"relu\",\n",
    "                    input_shape=(28, 28, 1),\n",
    "                    kernel_initializer=_tf_weights_loader(\"MNIST\", \"W\", \"CONV2D\", 2),\n",
    "                    bias_initializer=_tf_weights_loader(\"MNIST\", \"B\", \"CONV2D\", 2),\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            model.add(\n",
    "                Conv2D(\n",
    "                    1,\n",
    "                    kernel_size=(7, 7),\n",
    "                    activation=\"relu\",\n",
    "                    input_shape=(28, 28, 1),\n",
    "                    kernel_initializer=_kr_weights_loader(\"MNIST\", \"W\", \"CONV2D\"),\n",
    "                    bias_initializer=_kr_weights_loader(\"MNIST\", \"B\", \"CONV2D\"),\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        model.add(Conv2D(1, kernel_size=(7, 7), activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    if from_logits:\n",
    "        if load_init:\n",
    "            if is_tf23_keras24:\n",
    "                model.add(\n",
    "                    Dense(\n",
    "                        10,\n",
    "                        activation=\"linear\",\n",
    "                        kernel_initializer=_tf_weights_loader(\"MNIST\", \"W\", \"DENSE\", 2),\n",
    "                        bias_initializer=_tf_weights_loader(\"MNIST\", \"B\", \"DENSE\", 2),\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                model.add(\n",
    "                    Dense(\n",
    "                        10,\n",
    "                        activation=\"linear\",\n",
    "                        kernel_initializer=_kr_weights_loader(\"MNIST\", \"W\", \"DENSE\"),\n",
    "                        bias_initializer=_kr_weights_loader(\"MNIST\", \"B\", \"DENSE\"),\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            model.add(Dense(10, activation=\"linear\"))\n",
    "    else:\n",
    "        if load_init:\n",
    "            if is_tf23_keras24:\n",
    "                model.add(\n",
    "                    Dense(\n",
    "                        10,\n",
    "                        activation=\"softmax\",\n",
    "                        kernel_initializer=_tf_weights_loader(\"MNIST\", \"W\", \"DENSE\", 2),\n",
    "                        bias_initializer=_tf_weights_loader(\"MNIST\", \"B\", \"DENSE\", 2),\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                model.add(\n",
    "                    Dense(\n",
    "                        10,\n",
    "                        activation=\"softmax\",\n",
    "                        kernel_initializer=_kr_weights_loader(\"MNIST\", \"W\", \"DENSE\"),\n",
    "                        bias_initializer=_kr_weights_loader(\"MNIST\", \"B\", \"DENSE\"),\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    if loss_name == \"categorical_hinge\":\n",
    "        if loss_type == \"label\":\n",
    "            raise AttributeError(\"This combination of loss function options is not supported.\")\n",
    "        elif loss_type == \"function_losses\":\n",
    "            loss = keras.losses.categorical_hinge\n",
    "    elif loss_name == \"categorical_crossentropy\":\n",
    "        if loss_type == \"label\":\n",
    "            if from_logits:\n",
    "                raise AttributeError(\"This combination of loss function options is not supported.\")\n",
    "            else:\n",
    "                loss = loss_name\n",
    "        elif loss_type == \"function_losses\":\n",
    "            if from_logits:\n",
    "                if int(keras.__version__.split(\".\")[0]) == 2 and int(keras.__version__.split(\".\")[1]) >= 3:\n",
    "\n",
    "                    def categorical_crossentropy(y_true, y_pred):\n",
    "                        return keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "                    loss = categorical_crossentropy\n",
    "                else:\n",
    "                    raise NotImplementedError(\"This combination of loss function options is not supported.\")\n",
    "            else:\n",
    "                loss = keras.losses.categorical_crossentropy\n",
    "        elif loss_type == \"function_backend\":\n",
    "            if from_logits:\n",
    "\n",
    "                def categorical_crossentropy(y_true, y_pred):\n",
    "                    return keras.backend.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "                loss = categorical_crossentropy\n",
    "            else:\n",
    "                loss = keras.backend.categorical_crossentropy\n",
    "    elif loss_name == \"sparse_categorical_crossentropy\":\n",
    "        if loss_type == \"label\":\n",
    "            if from_logits:\n",
    "                raise AttributeError(\"This combination of loss function options is not supported.\")\n",
    "            else:\n",
    "                loss = loss_name\n",
    "        elif loss_type == \"function_losses\":\n",
    "            if from_logits:\n",
    "                if int(keras.__version__.split(\".\")[0]) == 2 and int(keras.__version__.split(\".\")[1]) >= 3:\n",
    "\n",
    "                    def sparse_categorical_crossentropy(y_true, y_pred):\n",
    "                        return keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "                    loss = sparse_categorical_crossentropy\n",
    "                else:\n",
    "                    raise AttributeError(\"This combination of loss function options is not supported.\")\n",
    "            else:\n",
    "                loss = keras.losses.sparse_categorical_crossentropy\n",
    "        elif loss_type == \"function_backend\":\n",
    "            if from_logits:\n",
    "\n",
    "                def sparse_categorical_crossentropy(y_true, y_pred):\n",
    "                    return keras.backend.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "                loss = sparse_categorical_crossentropy\n",
    "            else:\n",
    "                loss = keras.backend.sparse_categorical_crossentropy\n",
    "    elif loss_name == \"kullback_leibler_divergence\":\n",
    "        if loss_type == \"label\":\n",
    "            raise AttributeError(\"This combination of loss function options is not supported.\")\n",
    "        elif loss_type == \"function_losses\":\n",
    "            loss = keras.losses.kullback_leibler_divergence\n",
    "        elif loss_type == \"function_backend\":\n",
    "            raise AttributeError(\"This combination of loss function options is not supported.\")\n",
    "    elif loss_name == \"cosine_similarity\":\n",
    "        if loss_type == \"label\":\n",
    "            loss = loss_name\n",
    "        elif loss_type == \"function_losses\":\n",
    "            loss = keras.losses.cosine_similarity\n",
    "        elif loss_type == \"function_backend\":\n",
    "            loss = keras.backend.cosine_similarity\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Loss name not recognised.\")\n",
    "\n",
    "    model.compile(loss=loss, optimizer=keras.optimizers.Adam(lr=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "    # Get classifier\n",
    "    krc = KerasClassifier(model, clip_values=(0, 1), use_logits=from_logits)\n",
    "\n",
    "    return krc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier = get_image_classifier_kr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate adversarial samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jinie/anaconda3/envs/toolbox/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jinie/anaconda3/envs/toolbox/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jinie/anaconda3/envs/toolbox/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jinie/anaconda3/envs/toolbox/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jinie/anaconda3/envs/toolbox/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jinie/anaconda3/envs/toolbox/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attacker = FastGradientMethod(classifier, eps=0.05)\n",
    "x_train_adv = attacker.generate(x_train)\n",
    "x_test_adv = attacker.generate(x_test) # this takes about two minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile training data for detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_detector = np.concatenate((x_train, x_train_adv), axis=0)\n",
    "\n",
    "bgd = x_train\n",
    "clean = x_test\n",
    "anom = x_test_adv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial samples detecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f836d0f2fcc3451b9355c58559a9ce9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subset scanning:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.353407688374094, 13.346051838929732, 16.92527157653528, 10.54952468451945, 18.451540434417502, 16.144130778606844, 19.070703157390493, 9.179674981574411, 13.353154486970144, 6.831363209181577, 20.285431801291388, 16.144130778606844, 9.402902843498225, 16.144130778606844, 9.535351578695247, 13.353154486970144, 14.574494189585415, 9.71632945972361, 13.84536155052378, 14.135382712060528, 10.344461526373797, 10.802101477998978, 16.67790808665872, 11.335717703010879, 17.247487589450948, 10.250855796898612, 10.69144303671026, 19.171924803479335, 14.836814374925844, 6.884756236180809, 12.933600178439825, 6.119783321049607, 10.676841471143785, 19.070703157390493, 9.77371284381141, 12.564784632942692, 10.651069335266296, 5.123522406886183, 11.976359914476078, 5.354810405918406, 6.89899503578038, 12.089479697525201, 13.349492210173345, 17.163632841651445, 12.473350209495303, 14.911497069372816, 7.685283610329273, 16.015262206715676, 7.0330164563463, 15.256562525912395, 13.316636708208987, 7.697496039996333, 10.247044813772366, 13.423355464497352, 17.81907172785043, 16.92527157653528, 21.35368294228757, 15.687893345125257, 8.868900976171556, 6.467664216567518, 9.535351578695247, 6.1505134781391675, 8.183114565267054, 9.686478467164108, 10.34849255367057, 13.349492210173345, 15.807733551426454, 15.03613383907337, 9.67158375802016, 14.92726376701755, 7.398131504560549, 11.562677119236069, 19.763527333292433, 5.063101531114429, 5.748266745973256, 7.370127178405913, 10.247044813772366, 8.622400118959884, 6.390641601159778, 6.414580033330277, 9.711385081432832, 8.252812627081196, 16.187747966930495, 15.214073850968541, 7.628281262956198, 11.837010407296878, 9.179674981574411, 6.831363209181577, 11.572803499690124, 8.909535863925216, 10.818581401643419, 12.073241312615664, 10.344461526373797, 7.628281262956198, 5.97744280803388, 6.4775529731490735, 8.53920401147697, 7.685283610329273, 15.522738830505855, 10.34849255367057]\n",
      "[12.566202227757529, 17.81907172785043, 20.69698510734114, 11.856379123356005, 15.522738830505855, 22.421733866286235, 17.041710936426075, 9.880315936130003, 12.955105946298147, 7.127628691140173, 17.163632841651445, 12.13923135179104, 8.497461946253727, 14.173599748273357, 13.962446919730587, 9.775343571148737, 12.478246868547856, 13.192836508369304, 13.346051838929732, 11.5462440599945, 9.51558870711613, 12.301026956278335, 19.760631872260007, 15.256562525912395, 14.567077622149249, 8.142143872918664, 13.192836508369304, 16.582786151413305, 10.676841471143785, 10.554269206695443, 14.740254356811826, 11.347617362236646, 12.643230754456862, 17.81907172785043, 7.8202748569189895, 11.856379123356005, 10.35738410638477, 10.34849255367057, 16.754936303676704, 12.073241312615664, 11.496533491946511, 14.574494189585415, 9.284414205466346, 14.255257382280346, 13.962446919730587, 12.781283202319557, 9.234985555858513, 14.911497069372816, 7.6070369254842705, 9.77371284381141, 14.911497069372816, 7.621319945575685, 8.084580270709397, 12.473350209495303, 13.349492210173345, 12.955105946298147, 16.582786151413305, 22.421733866286235, 12.473350209495303, 9.037823437773316, 14.796263009121098, 6.596418254184652, 9.437749444225974, 10.142715900645694, 7.864791203521645, 14.173599748273357, 12.829160066660554, 16.037164555065388, 9.880315936130003, 12.992466435917246, 11.442421894434297, 7.852990395589182, 16.92527157653528, 7.628281262956198, 10.34849255367057, 7.697496039996333, 12.379218940621794, 14.255257382280346, 7.283538811074624, 4.311200059479942, 11.5462440599945, 7.9157019050215816, 16.582786151413305, 19.070703157390493, 7.086799874136679, 10.059466805453198, 8.03362083357628, 7.843946672562629, 9.71632945972361, 13.79799007156076, 8.980412046662387, 11.05519076760887, 11.23042218169307, 10.651069335266296, 10.344461526373797, 6.239123434273928, 9.234985555858513, 10.250855796898612, 11.873552857532372, 12.473350209495303]\n"
     ]
    }
   ],
   "source": [
    "detector = SubsetScanningDetector(classifier, bgd, layer=1)\n",
    "\n",
    "# clean_scors, adv_scores, dpwr = detector.scan(clean, clean)\n",
    "clean_scores, adv_scores, dpwr = detector.scan(clean, anom)\n",
    "\n",
    "print(clean_scores)\n",
    "print(adv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toolbox",
   "language": "python",
   "name": "toolbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
